# -*- coding: utf-8 -*-
"""CVAE Pickle Integration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kjTYokVclHLt5_whIf-u5gv5jitt_jD3
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# !pip install sounddevice
# !pip install noisereduce
# !apt-get install portaudio19-dev

from matplotlib import pyplot as plta
from matplotlib.offsetbox import OffsetImage, AnnotationBbox, TextArea
from matplotlib.colors import ListedColormap
import matplotlib
import numpy as np
import pandas as pd
import os
import pickle
import matplotlib.pyplot as plt

import librosa
import soundfile as sf
import sounddevice as sd
import noisereduce as nr

import torch
import torchvision
from torch import nn
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torch.utils import data
import torch.nn.functional as F
from torchvision import transforms
from torchvision.datasets import MNIST
from torchvision.utils import save_image
from sklearn.model_selection import train_test_split
from tqdm import tqdm, tqdm_notebook
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.cuda.empty_cache()

print('Training on',DEVICE)

SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

"""## DATA PRE-PROCESSING"""

# Load metadata
audio_dir = "D:/Development/audioGen/SythesizedBreathingM-3/data/BreathingSoundCapstoneData/CombinedDataset/CombinedDataset"
metadata_path = "D:/Development/audioGen/SythesizedBreathingM-3/data/Metadata.csv"
output_dir = "D:/Development/audioGen/SythesizedBreathingM-3/data/Spectrograms"

metadata_df = pd.read_csv(metadata_path)
metadata_df.set_index("filename", inplace=True)
unique_diagnoses = metadata_df['patient_diagnosis'].unique()

os.makedirs(output_dir, exist_ok=True)

"""PICKLE IMPLEMENTATION"""

# Label mappings for categorical metadata
# Label mappings for categorical metadata
breath_sound_dict = {
    "sex": {},
    "patient_diagnosis": {},
    "ap_pos": {},
    "lateral_pos": {}
}

# Store inverse mappings (for debugging or interpretation)
inv_breath_sound = {
    "patient_diagnosis": {},
    "ap_pos": {},
    "lateral_pos": {},
    "sex" : {}
}

metadata_df['ap_pos'] = metadata_df['ap_pos'].astype(str)
metadata_df['lateral_pos'] = metadata_df['lateral_pos'].astype(str)
metadata_df['sex'] = metadata_df['sex'].astype(str)


# Extract unique labels for encoding
unique_diagnoses = sorted(set(metadata_df['patient_diagnosis'].unique()))  # Unique diagnoses in metadata
unique_ap_pos = sorted(set(metadata_df['ap_pos'].unique()))  # Unique AP positions in metadata
unique_lateral_pos = sorted(set(metadata_df['lateral_pos'].unique()))  # Unique lateral positions in metadata
unique_sex = sorted(set(metadata_df['sex'].unique()))

# Assign unique integer values to each category
for i, diag in enumerate(unique_diagnoses):
    breath_sound_dict["patient_diagnosis"][diag] = i
    inv_breath_sound["patient_diagnosis"][i] = diag

for i, ap in enumerate(unique_ap_pos):
    breath_sound_dict["ap_pos"][ap] = i
    inv_breath_sound["ap_pos"][i] = ap

for i, lat in enumerate(unique_lateral_pos):
    breath_sound_dict["lateral_pos"][lat] = i
    inv_breath_sound["lateral_pos"][i] = lat

for i, lat in enumerate(unique_sex):
    breath_sound_dict["sex"][lat] = i
    inv_breath_sound["sex"][i] = lat

# Print dataset details
print("Sex Encoding:", breath_sound_dict["sex"])
print("Diagnosis Encoding:", inv_breath_sound["patient_diagnosis"])
print("AP Position Encoding:", inv_breath_sound["ap_pos"])
print("Lateral Position Encoding:", inv_breath_sound["lateral_pos"])

# Global variables
SAMPLE_RATE = 44100 # Value in most audio files
MAX_LENGTH = SAMPLE_RATE * 5 # Assuming audios are 8 seconds long
FRAME_SIZE = 2048
HOP_LENGTH = 256 # Lower val = higher res
N_MELS = 256
MIN_VAL = 0  # To normalize
MAX_VAL = 1

#File to audio signal
def load_audio(file_path):
    signal = librosa.load(file_path, sr=SAMPLE_RATE)[0]
    return signal

def apply_padding(array):
    if len(array) < MAX_LENGTH:
        num_missing_items = MAX_LENGTH - len(array)
        padded_array = np.pad(array, (num_missing_items // 2, num_missing_items // 2), mode='constant')
        return padded_array
    elif len(array) > MAX_LENGTH:
        center = len(array) // 2
        start = max(0, center - MAX_LENGTH // 2)
        end = min(len(array), start + MAX_LENGTH)
        trimmed_array = array[start:end]
        return trimmed_array
    return array

#Mel in dB scale
def extract_mel_spectrogram(signal, sample_rate=SAMPLE_RATE, frame_size=FRAME_SIZE, hop_length=HOP_LENGTH, n_mels=N_MELS):
    mel_spectrogram = librosa.feature.melspectrogram(
        y=signal, sr=sample_rate, n_fft=frame_size, hop_length=hop_length, n_mels=n_mels)
    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)
    return mel_spectrogram

def min_max_normalize(array):
    max_val = array.max()
    min_val = array.min()
    norm_array = (array - min_val) / (max_val - min_val)
    return norm_array, min_val, max_val

def denormalize(norm_array, original_min, original_max, min_val, max_val):
    array = (norm_array - min_val) / (max_val - min_val)
    array = array * (original_max - original_min) + original_min
    return array

# Convert mel spec to audio in power scale
def spec_to_audio(mel_spectrogram, original_min, original_max,sr=SAMPLE_RATE, hop_length=HOP_LENGTH, n_fft=FRAME_SIZE, n_iter=64):
    mel_db = denormalize(mel_spectrogram,original_min,original_max,0,1)
    mel_power = librosa.db_to_power(mel_db, ref=1.0)
    audio = librosa.feature.inverse.mel_to_audio(
        mel_power,
        sr=sr,
        hop_length=hop_length,
        n_fft=n_fft,
        n_iter=n_iter
    )
    return audio
# Plays audio
# Find out output_device_id value for ur device using print(sd.query_devices())
def play_audio(audio, output_device_id=4):
    Audio(audio, rate=41100)
    #sd.play(audio, samplerate=SAMPLE_RATE, device=output_device_id)
    #sd.wait()

#STFT instead
def compute_stft(signal, n_fft=FRAME_SIZE, hop_length=HOP_LENGTH):
    stft = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length)
    magnitude = np.abs(stft)
    phase = np.angle(stft)
    return magnitude, phase

def audio_from_stft(magnitude, phase, hop_length=HOP_LENGTH):
    phase = np.unwrap(phase, axis=-1)
    complex_stft = magnitude * np.exp(1j * phase)
    signal = librosa.istft(complex_stft, hop_length=hop_length)
    return signal

"""## DATA LOADING"""
def process_audio(file_name):
    base_name = os.path.basename(file_name)
    file_path = os.path.join(audio_dir, base_name)

    if not os.path.exists(file_path):
        print(f"File {file_name} not found!")
        return None
    signal = load_audio(file_path)
    padded_signal = apply_padding(signal)
    magnitude, phase = compute_stft(padded_signal)
    #print(magnitude.shape, phase.shape)
    print(phase)

    if base_name in metadata_df.index:
       metadata = metadata_df.loc[base_name].to_dict()
    else:
       metadata = {"age": None, "sex": None, "patient_diagnosis": None, "ap_pos": None, "lateral_pos": None}

    save_path = os.path.join(output_dir, base_name + ".pkl")
    with open(save_path, "wb") as f:
        pickle.dump({
        "magnitude": magnitude,
        "phase": phase,
        "metadata": metadata,
    }, f)

for file_name in metadata_df.index:
    file_path = os.path.join(audio_dir, file_name)
    process_audio(file_path)

"""Make sure audio works"""



file_path = "../data/Spectrograms/101_1b1_Pr_sc_Meditron.wav.pkl"
with open(file_path, "rb") as f:
  data = pickle.load(f)

metadata_for_sample = data['metadata']
phase_for_sample = data['phase']
magnitude_for_sample = data['magnitude']

og_min = data.get('original_min')
og_max = data.get('original_max')

age = metadata_for_sample.get("age", "Unknown")
sex_code = metadata_for_sample.get("sex", "Unknown")
diagnosis_code = metadata_for_sample.get("patient_diagnosis", "Unknown")
ap_pos_code = metadata_for_sample.get("ap_pos", "Unknown")
lateral_pos_code = metadata_for_sample.get("lateral_pos", "Unknown")

# Decode values using the inverse lookup
sex = inv_breath_sound['sex'].get(sex_code, sex_code)
diagnosis = inv_breath_sound['patient_diagnosis'].get(diagnosis_code, diagnosis_code)
ap_pos = inv_breath_sound['ap_pos'].get(ap_pos_code, ap_pos_code)
lateral_pos = inv_breath_sound['lateral_pos'].get(lateral_pos_code, lateral_pos_code)


metadata_str = f"Age: {age}\nSex: {sex}\nDiagnosis: {diagnosis}\nAP Pos: {ap_pos}\nLateral Pos: {lateral_pos}"

plt.imshow(np.squeeze(phase_for_sample))
#plt.imshow(np.squeeze(magnitude_for_sample), cmap="plasma")
print(phase_for_sample.max(), phase_for_sample.min())

plt.figtext(0.15, 0.7, metadata_str, fontsize=12, ha='right', va='top', bbox=dict(facecolor='white', alpha=0.7))

plt.show()

# from IPython.display import Audio
# audio = spec_to_audio(spectrogram_for_sample, og_min, og_max )
# #play_audio(audio)
# Audio(audio, rate=41100)

X = []
Y = []
file_paths = []
folder_path = "D:/Development/audioGen/SythesizedBreathingM-3/data/Spectrograms"
file_names = sorted(os.listdir(folder_path))
for file_name in file_names:
    file_path = os.path.join(folder_path, file_name)
    if file_name.endswith('.pkl'):
        with open(file_path, "rb") as f:
            data = pickle.load(f)
            magnitude = data['magnitude']
            phase = data['phase']
            metadata = data["metadata"]
            combined = np.concatenate([magnitude, phase], axis=1)

            X.append(combined)
            #X.append(phase)
            file_paths.append(file_path)
            # Create metadata-based label encoding
            sex = metadata.get("sex", "Unknown")
            sex_encoded = breath_sound_dict["sex"].get(sex,-1)

            diagnosis = metadata.get("patient_diagnosis", "Unknown")
            diagnosis_encoded = breath_sound_dict["patient_diagnosis"].get(diagnosis, 4)  # Automatically healthy if unknown

            ap_pos = metadata.get("ap_pos", "Unknown")
            ap_pos_encoded = breath_sound_dict["ap_pos"].get(ap_pos, 2)  # Automatically NaN if nothing

            lateral_pos = metadata.get("lateral_pos", "Unknown")
            lateral_pos_encoded = breath_sound_dict["lateral_pos"].get(lateral_pos, 2)  # Automatically NaN if nothing
            age = metadata.get('age')

            label = [
                float(age),
                float(sex_encoded),
                float(diagnosis_encoded),
                float(ap_pos_encoded),
                float(lateral_pos_encoded)
            ]
            Y.append(label)
            #Y.append(label)

#print(len(X), len(Y))
#print(X[0], Y[0])
# Convert to NumPy arrays
# Perform train/test split (80/20)
X_arr = np.array (X)
X_arr = np.array(X_arr)[..., np.newaxis]
Y_arr = np.array(Y)
print(X_arr.shape, Y_arr.shape)

X_arr = np.transpose(X_arr, (0, 3, 1, 2))  # Adjust to (batch, channels, height, width)
X_train, X_test, y_train, y_test =  train_test_split(X_arr, Y_arr, test_size=0.2)

#Data Shuffle

indices = np.arange(len(X_train))
print(indices.shape)
testIndices = np.arange(len(X_test))
np.random.shuffle(indices)
x_train = X_train[indices]
y_train = y_train[indices]

# More variables
batch_size = 8
learning_rate = 0.00001
hidden_size = 256 # Size of hidden layers
num_epochs = 10000
input_size = 1025 * 1724 # Make sure this matches actual size!
labels_length = 5 # 5 labels total

# Convert to tensors
# Create data loaders
x_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)

train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)
train_loader1 = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

x_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

test_dataset = torch.utils.data.TensorDataset(x_test_tensor, y_test_tensor)
test_loader1 = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

train_dataset = train_loader1
val_dataset = test_loader1

print(y_train_tensor.shape, x_train_tensor.shape, y_test_tensor.shape, x_test_tensor.shape)

audio_index = 152

metadata_for_sample = y_train[audio_index]


age = metadata_for_sample[0]
sex = 'M' if metadata_for_sample[1] == 0 else 'F'
diagnosis = inv_breath_sound['patient_diagnosis'].get(metadata_for_sample[2])
ap_pos = inv_breath_sound['ap_pos'].get(metadata_for_sample[3])
lateral_pos = inv_breath_sound['lateral_pos'].get(metadata_for_sample[4])

metadata_str = f"Age: {age}\nSex: {sex}\nDiagnosis: {diagnosis}\nAP Pos: {ap_pos}\nLateral Pos: {lateral_pos}"

#plt.imshow(np.squeeze(x_train[audio_index]), cmap="plasma")


#plt.figtext(0.05, 0.7, metadata_str, fontsize=12, ha='right', va='top', bbox=dict(facecolor='white', alpha=0.7))

#plt.show()

# from IPython.display import Audio
# mel_spectrogram = x_train[audio_index]
# audio = spec_to_audio(mel_spectrogram, -80,0)
# #play_audio(audio)
# Audio(audio, rate=41100)

# #sd.play(sound_audio, samplerate = sr)
# #sd.wait()

"""## Utility functions

The following functions will be used for visualizing the training progress, estimating the latent space, and reducing the dimensionality of the latent space in order to visualize it in a 2/3-dimensional space.
"""

# Utility functions
def master_encoding(labels):
        device = labels.device
        age = labels[:, 0]
        sex = labels[:, 1]
        diagnosis = labels[:, 2]
        ap_pos = labels[:, 3]
        lateral_pos = labels[:,4]
        # REPLACE NAN VALUES
        age = torch.nan_to_num(age, nan=0.0)  # Replace NaN in age with 0
        sex = torch.nan_to_num(sex, nan=1.0)  # Replace NaN in sex with 1
        diagnosis = torch.nan_to_num(diagnosis, nan=5)  # Replace NaN in diagnosis with healthy
        ap_pos = torch.nan_to_num(ap_pos, nan=2)  # Replace NaN
        lateral_pos = torch.nan_to_num(lateral_pos, nan=0)  # Replace NaN

        age_tensor = age.unsqueeze(1)
        sex_encoded_tensor = sex.unsqueeze(1)
        diagnosis_encoded_tensor = diagnosis.unsqueeze(1)
        ap_pos_encoded_tensor = ap_pos.unsqueeze(1)
        lateral_pos_encoded_tensor = lateral_pos.unsqueeze(1)

        # Concatenate all tensors along the second dimension (dim=1)
        encoded_labels = torch.cat([age_tensor, sex_encoded_tensor, diagnosis_encoded_tensor, ap_pos_encoded_tensor, lateral_pos_encoded_tensor], dim=1).to(device)
        return encoded_labels

def plot_gallery(images, h, w, n_row=3, n_col=6):
    plt.figure(figsize=(2 * n_col, 2 * n_row))
    for i in range(n_row * n_col):
        plt.subplot(n_row, n_col, i + 1)
        plt.axis("off")
        plt.imshow(images[i].reshape(h, w), cmap = "plasma")
    plt.show()

def vae_loss_fn(x, recon_x, mu, logvar, recon_scale=1, kl_scale=0.01): # CHANGED TO NOT RETURN NAN VALUES
    logvar = torch.clamp(logvar, min=-1, max=1)
    #MEAN REDUCTION TO REDUCE MASSIVE LOSSES
    reconstruction_loss = F.mse_loss(recon_x, x, reduction='mean')
    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_scale * reconstruction_loss + kl_scale * KLD


def evaluate(losses, autoencoder, dataloader, flatten=True):
    model = lambda x, y: autoencoder(x, y)[0]
    loss_sum = []
    inp, out = [],[]
    loss_fn = nn.MSELoss()
    for inputs, labels in dataloader:
        inputs = inputs.to(DEVICE)
        labels = master_encoding(labels).to(DEVICE)

        if flatten:
            inputs = inputs.view(inputs.size(0), input_size)

        outputs = model(inputs, labels)
        loss = loss_fn(inputs, outputs)
        loss_sum.append(loss)
        inp = inputs
        out = outputs

    # with torch.set_grad_enabled(False):
    #     plot_gallery([inp[0].detach().cpu(),out[0].detach().cpu()],256,1285,1,2)

    losses.append((sum(loss_sum)/len(loss_sum)).item())

"""# CVAE

In the following cells, the CVAE model will be implemented. Various attempts have been made by varying parameters and hyperparameters. The code will present one of the most efficient models in terms of generative and computational performance.
"""
class CVAE(nn.Module):
    def check_for_nan(tensor, name="tensor"):
        if torch.isnan(tensor).any():
            print(f"Warning: {name} contains NaN values")
        if torch.isinf(tensor).any():
            print(f"Warning: {name} contains Inf values")

    def __init__(self, input_size, hidden_size=256):
        super(CVAE, self).__init__()
        input_size_with_label = input_size + labels_length
        hidden_size += labels_length

        self.fc1 = nn.Linear(input_size_with_label, 512) #512)
        self.fc21 = nn.Linear(512, hidden_size)
        self.fc22 = nn.Linear(512, hidden_size)

        self.relu = nn.ReLU()

        self.fc3 = nn.Linear(hidden_size, 512)
        self.fc4 = nn.Linear(512, input_size)

    def encode(self, x, labels):
        x = x.view(-1, input_size)
        #print(x.shape, labels.shape)
        # Concatenate the input and labels along the feature dimension
        x = torch.cat((x, labels), 1)
        x = self.relu(self.fc1(x))
        # ADDED small constant for stability
        mu = self.fc21(x)
        logvar = self.fc22(x) + 1e-6
        return mu, logvar
# Modified to include ability to decode labels alongside latent vector
    def decode(self, z):
        #z = torch.cat((z,labels), 1)
        z = self.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(z))

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 *logvar)
        #CLAMPED TO ENSURE no NAN
        std = torch.clamp(std,min=1e-6)
        eps = torch.randn_like(std)
        return eps.mul(std).add_(mu)

    def forward(self,x, labels):
        mu, logvar = self.encode(x, labels)
        z = self.reparameterize(mu, logvar)
        x = self.decode(z)
        return x, mu, logvar

def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=num_epochs):
    validation_losses = []
    optim = torch.optim.Adam(net.parameters(), lr=1e-4)

    log_template = "\nEpoch {ep:03d} val_loss {v_loss:0.4f}"
    with tqdm(desc="epoch", total=epochs) as pbar_outer:
        for i in range(epochs):
            for batch, labels in dataloader:
                batch = batch.to(DEVICE)
                labels = master_encoding(labels).to(DEVICE)
                #check_for_nan(batch, "batch")
                #check_for_nan(labels, "labels")

                if flatten:
                    batch = batch.view(batch.size(0), -1)

                optim.zero_grad()
                x,mu,logvar = net(batch, labels)
                loss = vae_loss_fn(batch, x[:, :input_size], mu, logvar, recon_scale=1.0, kl_scale=0.01)
                loss.backward()

                # GRADIENT CLIPPING
                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)

                optim.step()
            evaluate(validation_losses, net, test_dataloader, flatten=True)
            pbar_outer.update(1)
            tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))
    plt.show()
    return validation_losses

cvae = CVAE(input_size).to(DEVICE)
history = train_cvae(cvae, train_dataset, val_dataset)
torch.save(cvae, 'D:/Development/audioGen/SythesizedBreathingM-3/code/checkpoints/cvaePickle2.pt')

cvae = torch.load('D:/Development/audioGen/SythesizedBreathingM-3/code/checkpoints/cvaePickle2.pt', weights_only=False)
cvae.eval()

val_loss = history
plt.figure(figsize=(15, 9))
plt.plot(val_loss, label="val_loss")
plt.legend(loc='best')
plt.xlabel("epochs")
plt.ylabel("loss")
plt.show()

import warnings
warnings.filterwarnings("ignore")

val_loss = history
plt.figure(figsize=(15, 9))
plt.plot(val_loss, label="val_loss")
plt.legend(loc='best')
plt.xlabel("epochs")
plt.ylabel("loss")
plt.show()

"""## SOUND GENERATION"""

# Make sure everything is on the same device as your model
DEVICE = next(cvae.parameters()).device  # Automatically gets cuda or cpu

audio_index_test = 5

# ORIGINAL
#makes sure output is 2D and detached from computational space and in numpy array
original = x_test_tensor[audio_index_test].view(256, 1285).detach().cpu().numpy()

# GENERATED
test_labels = master_encoding(y_test_tensor.to(DEVICE))
x_test_tensor = x_test_tensor.to(DEVICE)

cvae_latent_space, _ = cvae.encode(x_test_tensor, test_labels)
cvae_generation = cvae.decode(cvae_latent_space)
#makes sure output is 2D and detached from computational space and in numpy array
cvae_gen_array = cvae_generation[audio_index_test].view(256, 1285).detach().cpu().numpy()

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(original, cmap="plasma")
plt.title("ORIGINAL SOUND")

plt.subplot(1, 2, 2)
plt.imshow(cvae_gen_array, cmap="plasma")
plt.title("GENERATED WITH CVAE")
metadata_for_test_sample = y_test_tensor[audio_index_test]


metadata_for_test_sample = y_test_tensor[audio_index_test].cpu()

age_t = metadata_for_test_sample[0].item()
sex_t = 'M' if metadata_for_test_sample[1].item() == 0 else 'F'
diagnosis_t = inv_breath_sound['patient_diagnosis'].get(metadata_for_test_sample[2].item())
ap_pos_t = inv_breath_sound['ap_pos'].get(metadata_for_test_sample[3].item())
lateral_pos_t = inv_breath_sound['lateral_pos'].get(metadata_for_test_sample[4].item())

metadata_str_t = f"Age: {age_t}\nSex: {sex_t}\nDiagnosis: {diagnosis_t}\nAP Pos: {ap_pos_t}\nLateral Pos: {lateral_pos_t}"

plt.figtext(0.10, 0.7, metadata_str_t, fontsize=12, ha='right', va='top', bbox=dict(facecolor='white', alpha=0.7))
plt.show()

from IPython.display import Audio
audio = spec_to_audio(original, -80,0)
#play_audio(audio)
Audio(audio, rate=41100)

from IPython.display import Audio
audio = spec_to_audio(cvae_gen_array, -80,0)
#play_audio(audio)
Audio(audio, rate=41100)