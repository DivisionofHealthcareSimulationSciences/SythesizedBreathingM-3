{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce401729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07bfadb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Training on cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Training on',DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "697b9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ca7aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data frame w metadata and filenames\n",
    "all_data = pd.read_csv('../data/BreathingSoundCapstoneData/CombinedDataset/all_data.csv')\n",
    "\n",
    "# Create column in DF that encodes disease state\n",
    "all_data['diagnosis_flag'] = all_data['patient_diagnosis'].map(\n",
    "    {'Healthy': 0,\n",
    "     'COPD': 1,\n",
    "     'Pneumonia': 2, \n",
    "     'Asthma': 3,\n",
    "     'URTI': 4, \n",
    "     'Heart Failure': 5,\n",
    "     'Bronchiectasis': 6,\n",
    "     'Bronchiolitis': 7,\n",
    "     'Lung Fibrosis': 8,\n",
    "     'LRTI': 9,\n",
    "     'Plueral Effusion': 10\n",
    "    })\n",
    "\n",
    "# list of filenames\n",
    "filenames = list(all_data['filename'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add199f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050 # Standard val - resampling to this\n",
    "MAX_LENGTH = 80000 # Because most are 80,000 length\n",
    "FRAME_SIZE = 2048\n",
    "HOP_LENGTH = 256 # Lower val = higher res\n",
    "N_MELS = 256\n",
    "MIN_VAL = 0  # To normalize\n",
    "MAX_VAL = 1\n",
    "\n",
    "# Replace with your local path to all_audio_data\n",
    "AUDIO_DIR_PATH = 'D:/Development/audioGen/SythesizedBreathingM-3/data/BreathingSoundCapstoneData/CombinedDataset/CombinedDataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e070ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File to audio signal\n",
    "def load_audio(file_path, sample_rate=SAMPLE_RATE):\n",
    "    signal, sr = librosa.load(file_path, sr=None)\n",
    "    signal_resampled = librosa.resample(signal, orig_sr=sr, target_sr=sample_rate)\n",
    "    return signal_resampled\n",
    "\n",
    "# Most of the audio signals are 80000 samples long\n",
    "# Trim if >, pad if <\n",
    "def apply_padding(array):\n",
    "    if len(array) < MAX_LENGTH: \n",
    "        num_missing_items = MAX_LENGTH - len(array)\n",
    "        padded_array = np.pad(array, (num_missing_items // 2, num_missing_items // 2), mode='constant')\n",
    "        return padded_array\n",
    "    elif len(array) > MAX_LENGTH:\n",
    "        center = len(array) // 2\n",
    "        start = max(0, center - MAX_LENGTH // 2)\n",
    "        end = min(len(array), start + MAX_LENGTH)\n",
    "        trimmed_array = array[start:end] \n",
    "        return trimmed_array\n",
    "    return array\n",
    " \n",
    " # Mel spectrogram in db scale\n",
    "def extract_mel_spectrogram(signal, sample_rate=SAMPLE_RATE, frame_size=FRAME_SIZE, hop_length=HOP_LENGTH, n_mels=N_MELS):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=signal, sr=sample_rate, n_fft=frame_size, hop_length=hop_length, n_mels=n_mels)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    return mel_spectrogram\n",
    "\n",
    "# Normalize\n",
    "def min_max_normalize(array):\n",
    "    max = array.max()\n",
    "    min = array.min()\n",
    "    norm_array = (array - min) / (max - min)\n",
    "    return norm_array, min, max\n",
    "\n",
    "# Denormalize \n",
    "def denormalize(norm_array, original_min, original_max, min_val, max_val):\n",
    "    array = (norm_array - min_val) / (max_val - min_val)\n",
    "    array = array * (original_max - original_min) + original_min\n",
    "    return array\n",
    "\n",
    "# Convert mel spec to audio in power scale\n",
    "def spec_to_audio(mel_spectrogram, sr=SAMPLE_RATE, hop_length=HOP_LENGTH):\n",
    "    mel_spectrogram = librosa.db_to_power(mel_spectrogram, ref=np.max)\n",
    "    audio = librosa.feature.inverse.mel_to_audio(mel_spectrogram, sr=sr, hop_length=hop_length)\n",
    "    return audio\n",
    "\n",
    "# Plays audio\n",
    "# Find out output_device_id value for your device using print(sd.query_devices())\n",
    "def play_audio(audio, output_device_id=4):\n",
    "    sd.play(audio, samplerate=SAMPLE_RATE, device=output_device_id)\n",
    "    sd.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d26b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates 4 arrays\n",
    "# X: all spectrograms\n",
    "# X_norm: all spectrograms normalized\n",
    "# y: disease labels (as strings/names)\n",
    "# y_encoded: encoded disease labels (0-10)\n",
    "# Use X_norm and y_encoded for training\n",
    "X =[]\n",
    "y = []\n",
    "y_encoded = []\n",
    "X_norm = []\n",
    "X_min_max = []\n",
    "for idx, row in all_data.iterrows():\n",
    "    filename = row['filename']\n",
    "    diagnosis = row['patient_diagnosis']\n",
    "    diagnosis_encoded = row['diagnosis_flag']\n",
    "    file_path = os.path.join(AUDIO_DIR_PATH, filename)\n",
    "    signal = load_audio(file_path)\n",
    "    padded_signal = apply_padding(signal)\n",
    "    mel_spec = extract_mel_spectrogram(padded_signal)\n",
    "    norm_mel_spec, original_min, original_max = min_max_normalize(mel_spec)\n",
    "\n",
    "    X.append(mel_spec)\n",
    "    X_norm.append(norm_mel_spec)\n",
    "    y.append(diagnosis)\n",
    "    y_encoded.append(diagnosis_encoded)\n",
    "    X_min_max.append([original_min, original_max])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a3e3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays, add more dimensions (not sure of purpose of this, I think to make it correct dimesions for NN layers)\n",
    "# Perform train/test split (80/20)\n",
    "X_arr = np.array(X_norm)\n",
    "X_arr = X_arr[..., np.newaxis] \n",
    "X_arr = np.transpose(X_arr, (0, 3, 1, 2))\n",
    "y_arr = np.array(y_encoded)\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X_arr, y_arr, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc28482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More variables\n",
    "batch_size = 64\n",
    "learning_rate = 0.0005 \n",
    "hidden_size = 256 # Size of hidden layers\n",
    "num_epochs = 10**6\n",
    "input_size = 256 * 313 # Make sure this matches actual size\n",
    "labels_length = 12 # 11 labels total, not sure why we have to add 1 here, they did this in the og code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ca9b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([820]) torch.Size([820, 1, 256, 313]) torch.Size([205]) torch.Size([205, 1, 256, 313])\n"
     ]
    }
   ],
   "source": [
    "# Convert to tensors\n",
    "# Create data loaders\n",
    "x_train_tensor = torch.tensor(X_train)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader1 = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x_test_tensor = torch.tensor(X_test)\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader1 = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_dataset = train_loader1\n",
    "val_dataset = test_loader1\n",
    "\n",
    "print(y_train_tensor.shape, x_train_tensor.shape, y_test_tensor.shape, x_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14575a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, max_x):\n",
    "    return torch.eye(max_x + 1)[x]\n",
    "\n",
    "def plot_gallery(images, h, w, n_row=3, n_col=6):\n",
    "    plt.figure(figsize=(2 * n_col, 2 * n_row))\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(images[i].reshape(h, w), cmap = \"plasma\")\n",
    "    plt.show()\n",
    "\n",
    "def vae_loss_fn(x, recon_x, mu, logvar):\n",
    "    reconstruction_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return reconstruction_loss + KLD\n",
    "\n",
    "def evaluate(losses, autoencoder, dataloader, flatten=True):\n",
    "    model = lambda x, y: autoencoder(x, y)[0]    \n",
    "    loss_sum = []\n",
    "    inp, out = [],[]\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = one_hot(labels, 11).to(DEVICE)\n",
    "\n",
    "        if flatten:\n",
    "            inputs = inputs.view(inputs.size(0), input_size)\n",
    "\n",
    "        outputs = model(inputs, labels)\n",
    "        loss = loss_fn(inputs, outputs)            \n",
    "        loss_sum.append(loss)\n",
    "        inp = inputs\n",
    "        out = outputs\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        plot_gallery([inp[0].detach().cpu(),out[0].detach().cpu()],256,313,1,2)    \n",
    "\n",
    "    losses.append((sum(loss_sum)/len(loss_sum)).item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2383665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=256):\n",
    "        super(CVAE, self).__init__()\n",
    "        input_size_with_label = input_size + labels_length\n",
    "        hidden_size += labels_length\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size_with_label, 512)\n",
    "        self.fc21 = nn.Linear(512, hidden_size)\n",
    "        self.fc22 = nn.Linear(512, hidden_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size, 512)\n",
    "        self.fc4 = nn.Linear(512, input_size)\n",
    "    \n",
    "    def encode(self, x, labels):\n",
    "        x = x.view(-1, input_size)\n",
    "        x = torch.cat((x, labels), 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc21(x), self.fc22(x)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        z = self.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(z))\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 *logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "        \n",
    "    def forward(self,x, labels):\n",
    "        mu, logvar = self.encode(x, labels)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x = self.decode(z)\n",
    "        return x, mu, logvar\n",
    "\n",
    "def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=num_epochs):\n",
    "    validation_losses = []\n",
    "    optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n",
    "        for i in range(epochs):\n",
    "            for batch, labels in dataloader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                labels = one_hot(labels,11).to(DEVICE)\n",
    "\n",
    "                if flatten:\n",
    "                    batch = batch.view(batch.size(0), input_size)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                x,mu,logvar = net(batch, labels)\n",
    "                loss = vae_loss_fn(batch, x[:, :input_size], mu, logvar)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            evaluate(validation_losses, net, test_dataloader, flatten=True)\n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n",
    "    plt.show()\n",
    "    return validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "039f20a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae = CVAE(input_size).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1a87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/1000000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "history = train_cvae(cvae, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5855c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m9\u001b[39m))\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(val_loss, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "val_loss = history\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(val_loss, label=\"val_loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
